{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bO5sgu_jSjTR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, matthews_corrcoef, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UL3LRlywTIyw"
      },
      "outputs": [],
      "source": [
        "## ********** data preprocessing functions **********\n",
        "\n",
        "def clean_data(X):\n",
        "  features_names = list(X)\n",
        "  return X, features_names\n",
        "\n",
        "def clean_metadata(meta_df):\n",
        "  new_index = np.array(meta_df.index)\n",
        "  k = 0\n",
        "  for i, row in meta_df.iterrows():\n",
        "    if row['Geography'] == 'Fiji':\n",
        "      new_index[k] = i + '_profiled_Metaphlan3'\n",
        "    elif row['Geography'] == 'Nunavik':\n",
        "      new_index[k] = 'X' + i\n",
        "    k += 1\n",
        "  meta_df = meta_df.set_index(new_index)\n",
        "  meta_df.columns = ['Age', 'Community', 'Sex', 'SampleIDC', 'Region', 'Lifestyle', 'Geography', 'Coast', 'Community size']\n",
        "  print('available metadata:', list(meta_df))\n",
        "  return meta_df\n",
        "\n",
        "def preprocess_data(X, metadata):\n",
        "  # assert that all samples are described in the metadata :\n",
        "  y = [] #prepare labels list \n",
        "  for sample_name in X.index:\n",
        "      if sample_name in metadata.index:\n",
        "          # get sample metadata and add the corresponding label in y\n",
        "          diet = metadata.loc[sample_name, 'Lifestyle']\n",
        "          y.append(diet)\n",
        "      else:\n",
        "          # if sample not described in metadata : remove it from data matrix\n",
        "          X = X.drop(sample_name)\n",
        "  print('classes in y:', list(dict.fromkeys(y)))\n",
        "  features_names = list(X)\n",
        "  y = np.array(y)\n",
        "  return X, y\n",
        "\n",
        "def get_hash(df):\n",
        "  # returns a hash value for a dataframe\n",
        "  # used to ensure that the data is the same than in the original experiment run\n",
        "  # idea from https://death.andgravity.com/stable-hashing\n",
        "  assert isinstance(df, pd.DataFrame)\n",
        "  json_dump = df.to_json(orient='split', date_format='epoch', double_precision=10, force_ascii=True, date_unit='ms', lines=False, index=True, indent=None)\n",
        "  return hashlib.md5(json_dump.encode('utf-8')).digest().hex()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pJ3Bw6_9N1Hb"
      },
      "outputs": [],
      "source": [
        "## ********** machine learning utils functions **********\n",
        "\n",
        "dir = ''\n",
        "splits_subdirectory = os.path.join(dir, 'splits')\n",
        "results_subdirectory = os.path.join(dir, 'grid_search_results')\n",
        "\n",
        "def generate_splits(X, y, n_splits, test_size, seed):\n",
        "  if not os.path.exists(splits_subdirectory):\n",
        "    os.makedirs(splits_subdirectory)\n",
        "  for id in range(n_splits):\n",
        "    random_state = seed + id\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    np.savez(os.path.join(splits_subdirectory,'split_'+str(id)), X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
        "    print('split {} saved in {}'.format(id, os.path.join(splits_subdirectory,'split_'+str(id))))\n",
        "\n",
        "def grid_search_on_split(split_id, classifier, hyperparameters_grid):\n",
        "  X_train, X_test, y_train, y_test = load_split(split_id)\n",
        "  grid = GridSearchCV(estimator=classifier, param_grid=hyperparameters_grid, verbose=20, n_jobs=4)\n",
        "  grid_result = grid.fit(X_train, y_train)\n",
        "  print('grid-Search done!\\n')\n",
        "  best_params = grid_result.best_params_\n",
        "  save_best_params(best_params, split_id)\n",
        "  print('best params:', best_params)\n",
        "  results = pd.DataFrame(grid_result.cv_results_) # convert GS results to a pandas dataframe\n",
        "  save_grid_search_results(results, split_id)\n",
        "\n",
        "def load_split(split_id):\n",
        "  split_filename = os.path.join(splits_subdirectory, 'split_{}.npz'.format(split_id))\n",
        "  split = np.load(split_filename)\n",
        "  X_train, X_test = split['X_train'], split['X_test']\n",
        "  y_train, y_test = split['y_train'], split['y_test']\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "def clean_splits_files(n_splits):\n",
        "  for split_id in range(n_splits):\n",
        "    split_filename = os.path.join(splits_subdirectory, 'split_{}.npz'.format(split_id))\n",
        "    os.remove(split_filename)\n",
        "  os.rmdir(splits_subdirectory)\n",
        "\n",
        "def save_best_params(best_params, split_id):\n",
        "  save_params_path = os.path.join(results_subdirectory,'best_params_{}.pkl'.format(split_id))\n",
        "  if not os.path.exists(results_subdirectory):\n",
        "    os.makedirs(results_subdirectory)\n",
        "  with open(save_params_path, 'wb') as f:\n",
        "    pickle.dump(best_params, f)\n",
        "  print('grid search best parameters for split {} saved in {}'.format(split_id, save_params_path))  \n",
        "\n",
        "def load_best_params(split_id):\n",
        "  best_params_filename = os.path.join(results_subdirectory,'best_params_{}.pkl'.format(split_id))\n",
        "  with open(best_params_filename, \"rb\") as f:\n",
        "    best_params = pickle.load(f)\n",
        "  return best_params\n",
        "\n",
        "def save_grid_search_results(results, split_id):\n",
        "  save_result_path = os.path.join(results_subdirectory,'results_{}.csv'.format(split_id))\n",
        "  if not os.path.exists(results_subdirectory):\n",
        "    os.makedirs(results_subdirectory)\n",
        "  results.to_csv(save_result_path)\n",
        "  print('grid search results for split {} saved in {}.csv'.format(split_id, save_result_path))  \n",
        "\n",
        "def load_grid_search_results(split_id):\n",
        "  results_filename = os.path.join(results_subdirectory,'results_{}.csv'.format(split_id))\n",
        "  best_params = pd.read_csv(results_filename)\n",
        "  return best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J1Ql7ihtZi51"
      },
      "outputs": [],
      "source": [
        "## ********** results vizualisation functions **********\n",
        "\n",
        "def evaluate_predictions(y_test, pred):\n",
        "  print('random_forest accuracy: ', round(accuracy_score(y_test, pred), 3))\n",
        "  print('random_forest balanced_accuracy_score: ', round(balanced_accuracy_score(y_test, pred), 3))\n",
        "  print('random_forest matthews_correlation coefficient: ', round(matthews_corrcoef(y_test, pred), 3))\n",
        "  conf_labels = ['Westernized', 'Nunavik', 'Non Westernized']\n",
        "  conf = confusion_matrix(y_test, pred, labels=conf_labels)\n",
        "  confusion = conf/conf.sum()\n",
        "  print('\\n\\n      # random forest confusion matrix :')\n",
        "  print('                                              predicted values')\n",
        "  print('                                  {}    |      {}     |  {}  | '.format(conf_labels[0], conf_labels[1], conf_labels[2]))\n",
        "  print('                                ---------------------------------------------------------')\n",
        "  print('   true     {}       |      {:.4f}      |      {:.4f}      |      {:.4f}       |'.format('Westernized', confusion[0,0], confusion[0,1], confusion[0,2]))\n",
        "  print('  values                        ---------------------------------------------------------')\n",
        "  print('            {}           |      {:.4f}      |      {:.4f}      |      {:.4f}       |'.format('Nunavik', confusion[1,0], confusion[1,1], confusion[1,2]))\n",
        "  print('                                ---------------------------------------------------------')\n",
        "  print('            {}   |      {:.4f}      |      {:.4f}      |      {:.4f}       |'.format('Non Westernized', confusion[2,0], confusion[2,1], confusion[2,2]))\n",
        "  print('                                ---------------------------------------------------------')\n",
        "\n",
        "def display_top_features(features_importance, features_names, n_top_feat):\n",
        "  result = {features_names[i] : round(features_importance[i],4) for i in range(len(features_names))}\n",
        "  res = {k: v for k, v in sorted(result.items(), key=lambda item: item[1], reverse=True)}\n",
        "  rank = 0\n",
        "  print('  rank    feature {:130s}'.format(''), 'importance')\n",
        "  for k in res:\n",
        "    rank += 1\n",
        "    if rank <= n_top_feat:\n",
        "      print('  {:3d}     {:140s}'.format(rank, k), res[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jC3Ec6TTchN",
        "outputId": "9b06a2ba-e72e-4266-9106-92d378ddbf91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "available metadata: ['Age', 'Community', 'Sex', 'SampleIDC', 'Region', 'Lifestyle', 'Geography', 'Coast', 'Community size']\n",
            "                                           Age  ... Community size\n",
            "SchirmerM_2016_G89275_profiled_Metaphlan3   20  ...    Netherlands\n",
            "SchirmerM_2016_G89250_profiled_Metaphlan3   22  ...    Netherlands\n",
            "SchirmerM_2016_G89187_profiled_Metaphlan3   27  ...    Netherlands\n",
            "SchirmerM_2016_G89182_profiled_Metaphlan3   21  ...    Netherlands\n",
            "SchirmerM_2016_G89134_profiled_Metaphlan3   21  ...    Netherlands\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "classes: ['Westernized', 'Non Westernized', 'Nunavik']\n",
            "classes in y: ['Westernized', 'Non Westernized', 'Nunavik']\n",
            "# of samples: 456\n",
            "# of features: 714\n",
            "split 0 saved in splits/split_0\n",
            "split 1 saved in splits/split_1\n",
            "split 2 saved in splits/split_2\n",
            "split 3 saved in splits/split_3\n",
            "split 4 saved in splits/split_4\n",
            "split 5 saved in splits/split_5\n",
            "split 6 saved in splits/split_6\n",
            "split 7 saved in splits/split_7\n",
            "split 8 saved in splits/split_8\n",
            "split 9 saved in splits/split_9\n",
            "split 0\n",
            "best params: {'n_estimators': 100}\n",
            "len(all_pred) 137\n",
            "split 1\n",
            "best params: {'n_estimators': 100}\n",
            "len(all_pred) 274\n",
            "split 2\n",
            "best params: {'n_estimators': 1000}\n",
            "len(all_pred) 411\n",
            "split 3\n",
            "best params: {'n_estimators': 1000}\n",
            "len(all_pred) 548\n",
            "split 4\n",
            "best params: {'n_estimators': 100}\n",
            "len(all_pred) 685\n",
            "split 5\n",
            "best params: {'n_estimators': 1000}\n",
            "len(all_pred) 822\n",
            "split 6\n",
            "best params: {'n_estimators': 1000}\n",
            "len(all_pred) 959\n",
            "split 7\n",
            "best params: {'n_estimators': 100}\n",
            "len(all_pred) 1096\n",
            "split 8\n",
            "best params: {'n_estimators': 100}\n",
            "len(all_pred) 1233\n",
            "split 9\n",
            "best params: {'n_estimators': 100}\n",
            "len(all_pred) 1370\n",
            "sum of importances for each split: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "random_forest accuracy:  0.936\n",
            "random_forest balanced_accuracy_score:  0.907\n",
            "random_forest matthews_correlation coefficient:  0.885\n",
            "\n",
            "\n",
            "      # random forest confusion matrix :\n",
            "                                              predicted values\n",
            "                                  Westernized    |      Nunavik     |  Non Westernized  | \n",
            "                                ---------------------------------------------------------\n",
            "   true     Westernized       |      0.1635      |      0.0569      |      0.0000       |\n",
            "  values                        ---------------------------------------------------------\n",
            "            Nunavik           |      0.0036      |      0.5985      |      0.0015       |\n",
            "                                ---------------------------------------------------------\n",
            "            Non Westernized   |      0.0015      |      0.0007      |      0.1737       |\n",
            "                                ---------------------------------------------------------\n",
            "  rank    feature                                                                                                                                    importance\n",
            "    1     s__Prevotella_sp_AM42_24                                                                                                                     0.0337\n",
            "    2     s__Flavonifractor_plautii                                                                                                                    0.0219\n",
            "    3     s__.Collinsella._massiliensis                                                                                                                0.0213\n",
            "    4     s__Enorma_massiliensis                                                                                                                       0.0212\n",
            "    5     s__Aeriscardovia_aeriphila                                                                                                                   0.0202\n",
            "    6     s__Lawsonibacter_asaccharolyticus                                                                                                            0.02\n",
            "    7     s__Bacteroides_uniformis                                                                                                                     0.0179\n",
            "    8     s__Clostridium_bolteae                                                                                                                       0.0161\n",
            "    9     s__Clostridium_leptum                                                                                                                        0.0161\n",
            "   10     s__Eggerthella_lenta                                                                                                                         0.0144\n",
            "   11     s__Clostridium_innocuum                                                                                                                      0.0139\n",
            "   12     s__Butyrivibrio_crossotus                                                                                                                    0.0137\n",
            "   13     s__Bifidobacterium_longum                                                                                                                    0.0135\n",
            "   14     s__Alistipes_finegoldii                                                                                                                      0.013\n",
            "   15     s__Prevotella_sp_885                                                                                                                         0.0129\n",
            "   16     s__Roseburia_sp_CAG_471                                                                                                                      0.0129\n",
            "   17     s__Eubacterium_hallii                                                                                                                        0.0127\n",
            "   18     s__Bacteroides_vulgatus                                                                                                                      0.0115\n",
            "   19     s__Collinsella_stercoris                                                                                                                     0.0108\n",
            "   20     s__Ruminococcus_torques                                                                                                                      0.0101\n",
            "   21     s__Dorea_formicigenerans                                                                                                                     0.01\n",
            "   22     s__Anaerotruncus_colihominis                                                                                                                 0.01\n",
            "   23     s__Enterorhabdus_caecimuris                                                                                                                  0.0095\n",
            "   24     s__Alistipes_putredinis                                                                                                                      0.0095\n",
            "   25     s__Leuconostoc_citreum                                                                                                                       0.0095\n",
            "   26     s__Anaerostipes_hadrus                                                                                                                       0.0095\n",
            "   27     s__Butyrivibrio_sp_CAG_318                                                                                                                   0.0094\n",
            "   28     s__Proteobacteria_bacterium_CAG_139                                                                                                          0.0089\n",
            "   29     s__Olsenella_scatoligenes                                                                                                                    0.0088\n",
            "   30     s__Parabacteroides_distasonis                                                                                                                0.0088\n"
          ]
        }
      ],
      "source": [
        "## ********** running experiments **********\n",
        "\n",
        "## ***** load data and metadata *****\n",
        "data_filename = 'especes_metaphlan3.csv'\n",
        "X = pd.read_csv(data_filename, index_col=0)\n",
        "X, features_names = clean_data(X)\n",
        "# ensure that the data file is the same than in original experiment run :\n",
        "assert get_hash(X) == '76db802436148af02fdc9d5ed48d7e5d'\n",
        "\n",
        "meta_filename = 'Metadata_samples_all_controls_2020-09-02_V6.csv'\n",
        "meta_df = pd.read_csv(meta_filename,sep=';',index_col=0)\n",
        "# ensure that the metadata file is the same than in original experiment run :\n",
        "assert get_hash(meta_df) == '53c67bf4e333301df1e90cddea472ef8'\n",
        "metadata = clean_metadata(meta_df)\n",
        "print(metadata.head())\n",
        "\n",
        "print('classes:', list(dict.fromkeys(metadata['Lifestyle'])))\n",
        "\n",
        "X, y = preprocess_data(X, metadata)\n",
        "\n",
        "print('# of samples:', X.shape[0])\n",
        "print('# of features:', X.shape[1])\n",
        "assert np.array_equal(X.shape[0], len(y))\n",
        "\n",
        "## ***** generate splits of the data matrix *****\n",
        "n_splits = 10\n",
        "test_size = 0.3\n",
        "seed = 1\n",
        "generate_splits(X, y, n_splits, test_size, seed)\n",
        "\n",
        "## ***** grid search tuning on splits *****\n",
        "#random_forest_n_estims = [1, 10, 100, 1000]\n",
        "#random_forest_param_grid = {'n_estimators' : random_forest_n_estims}\n",
        "#for split_id in range(n_splits):\n",
        "#  classifier = RandomForestClassifier(random_state=1)\n",
        "#  grid_search_on_split(split_id, classifier, random_forest_param_grid)\n",
        "\n",
        "## ***** compute classifiers average performances and features importances *****\n",
        "all_pred, all_y_test = [], []\n",
        "sum_imp = []\n",
        "\n",
        "for split_id in range(n_splits):\n",
        "  print('split', split_id)\n",
        "  X_train, X_test, y_train, y_test = load_split(split_id)\n",
        "  classifier = RandomForestClassifier(random_state=1) #define classifier\n",
        "  best_params = load_best_params(split_id)\n",
        "  classifier.set_params(**best_params)\n",
        "  print('best params:', best_params)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  all_pred.extend(y_pred)\n",
        "  print('len(all_pred)', len(all_pred))\n",
        "  all_y_test.extend(y_test)\n",
        "  features_importance = classifier.feature_importances_\n",
        "  assert len(features_importance) == X.shape[1] == len(features_names)\n",
        "  sum_imp.append(features_importance.sum())\n",
        "  if split_id == 0:\n",
        "    all_imp = features_importance\n",
        "  else:\n",
        "    all_imp = np.add(all_imp, features_importance)\n",
        "\n",
        "## ***** display results *****\n",
        "print('sum of importances for each split:', sum_imp) #check that it is always 1\n",
        "evaluate_predictions(y_test=all_y_test, pred=all_pred)\n",
        "display_top_features(features_importance, features_names, 30)\n",
        "\n",
        "## ***** cleaning *****\n",
        "clean_splits_files(n_splits)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "nunavik-ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
